{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56ded2ed-fc34-4976-a341-37071aa5beab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def load_data(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def create_dataframe(data):\n",
    "    return pd.DataFrame(data, columns=['text', 'intent'])\n",
    "\n",
    "def prepare_tokenizer(model_name):\n",
    "    return BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def encode_texts(tokenizer, texts, max_length=50):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
    "\n",
    "def prepare_labels(labels):\n",
    "    le = LabelEncoder()\n",
    "    labels_encoded = le.fit_transform(labels)\n",
    "    return to_categorical(labels_encoded), le\n",
    "\n",
    "# Load and prepare data\n",
    "data = load_data('/Users/bishalthapa/Desktop/2024/Deep Learning/Project/Code/DeepLearningProject/clinc150_uci/data_full.json')\n",
    "train_df = create_dataframe(data['train'])\n",
    "val_df = create_dataframe(data['val'])\n",
    "oos_train_df = create_dataframe(data['oos_train'])\n",
    "oos_val_df = create_dataframe(data['oos_val'])\n",
    "\n",
    "# Combine all labels for consistent encoding\n",
    "all_labels = np.concatenate([train_df['intent'], val_df['intent'], oos_train_df['intent'], oos_val_df['intent']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f473c9ae-a541-4074-8bca-bcee5abbdf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tokenizer\n",
    "tokenizer = prepare_tokenizer('bert-base-uncased')\n",
    "\n",
    "# Encode texts\n",
    "train_encodings = encode_texts(tokenizer, train_df['text'].tolist())\n",
    "val_encodings = encode_texts(tokenizer, val_df['text'].tolist())\n",
    "oos_train_encodings = encode_texts(tokenizer, oos_train_df['text'].tolist())\n",
    "oos_val_encodings = encode_texts(tokenizer, oos_val_df['text'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fec9d27c-b1fe-49e2-9cdb-f38fee492075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Prepare labels\n",
    "train_labels, label_encoder = prepare_labels(train_df['intent'])\n",
    "val_labels, _ = prepare_labels(val_df['intent'])\n",
    "oos_train_labels, _ = prepare_labels(oos_train_df['intent'])\n",
    "oos_val_labels, _ = prepare_labels(oos_val_df['intent'])\n",
    "\n",
    "# Load BERT model\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af49e97e-ff69-455c-b3af-754765ad75ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x372772f10>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)\n\u001b[1;32m      3\u001b[0m loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39mloss, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:1563\u001b[0m, in \u001b[0;36mTFPreTrainedModel.compile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001b[0m\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;66;03m# This argument got renamed, we need to support both versions\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps_per_execution\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m parent_args:\n\u001b[0;32m-> 1563\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m   1564\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m   1565\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m   1566\u001b[0m         metrics\u001b[38;5;241m=\u001b[39mmetrics,\n\u001b[1;32m   1567\u001b[0m         loss_weights\u001b[38;5;241m=\u001b[39mloss_weights,\n\u001b[1;32m   1568\u001b[0m         weighted_metrics\u001b[38;5;241m=\u001b[39mweighted_metrics,\n\u001b[1;32m   1569\u001b[0m         run_eagerly\u001b[38;5;241m=\u001b[39mrun_eagerly,\n\u001b[1;32m   1570\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39msteps_per_execution,\n\u001b[1;32m   1571\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1572\u001b[0m     )\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1574\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m   1575\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m   1576\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1582\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1583\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tf_keras/src/optimizers/__init__.py:335\u001b[0m, in \u001b[0;36mget\u001b[0;34m(identifier, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get(\n\u001b[1;32m    331\u001b[0m         config,\n\u001b[1;32m    332\u001b[0m         use_legacy_optimizer\u001b[38;5;241m=\u001b[39muse_legacy_optimizer,\n\u001b[1;32m    333\u001b[0m     )\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not interpret optimizer identifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    337\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x372772f10>"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
